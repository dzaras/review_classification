{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Topic Modeling - Word Embeddings_and_Clustering_9-15-21.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7To7pe6vUx_1",
        "kVnPb5rhWgrt",
        "5osdPwa7YEMd",
        "MgA3PBsebSSm",
        "hPkyePM2y1i-",
        "-JaJ10xvN-Gy",
        "S4g3HM9RL2EU",
        "5ltBVizKYZ3E"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzaras/review_classification/blob/main/Topic_Modeling_Word_Embeddings_and_Clustering_9_15_21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P6G6g_iIduV",
        "outputId": "64124732-473d-4cae-9392-b81cd138917d"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install numpy \n",
        "!pip install pandas\n",
        "!pip install gensim\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvBc305rIyv3",
        "outputId": "6c3b501f-e2a5-4e6b-e4f7-0385810a538f"
      },
      "source": [
        "# download relevant parts of NLTK\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbVMgSbBKgEV"
      },
      "source": [
        "# Overview\n",
        "## 1. Topic Modeling with LDA\n",
        "\n",
        "## 2. Word Embeddings\n",
        "\n",
        "\n",
        "## 3. (Optional) Sentence/Document Clustering Using Word Embeddings\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmU93h9ALxqO",
        "outputId": "d5d7494d-0244-445f-f2f8-acd382c03823"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Here we use 20-Newsgroups dataset (http://qwone.com/~jason/20Newsgroups/) for this example. \n",
        "# This version of the dataset contains about 11k newsgroups posts from 20 different topics. \n",
        "# This is available as https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json\n",
        "\n",
        "#raw_data = pd.read_csv('https://raw.githubusercontent.com/dzaras/review_classification/main/newspaper_data_3_labels_updated.csv' , encoding='latin-1')\n",
        "raw_data = pd.read_csv('https://raw.githubusercontent.com/dzaras/review_classification/main/pro_reviews_female_lead_actors.csv', encoding = 'latin-1')\n",
        "#raw_data = pd.read_csv('https://raw.githubusercontent.com/dzaras/review_classification/main/pro_reviews_2.9.3.csv', encoding = 'latin-1')\n",
        "print(raw_data.rating_names.unique())\n",
        "\n",
        "# raw_data = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "#print(raw_data.target_names.unique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Unfavorable' 'Mixed' 'Favorable' 'No Rating']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0vlnN87q4pK",
        "outputId": "aa4ad9e7-2f61-4d5c-9515-5a16cba452ad"
      },
      "source": [
        "raw_data.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "doc.id                 int64\n",
              "content               object\n",
              "primaryTitle          object\n",
              "lead.actor.feamle      int64\n",
              "female.director      float64\n",
              "media.name            object\n",
              "year                   int64\n",
              "rating                object\n",
              "rating_names          object\n",
              "no.rating              int64\n",
              "pub.type2             object\n",
              "newspaper.name        object\n",
              "elite.np             float64\n",
              "author.female        float64\n",
              "genres                object\n",
              "first.genre           object\n",
              "crime                  int64\n",
              "drama                  int64\n",
              "action                 int64\n",
              "horror                 int64\n",
              "comedy                 int64\n",
              "mystery                int64\n",
              "romance                int64\n",
              "adventure              int64\n",
              "animation              int64\n",
              "thriller               int64\n",
              "fantasy                int64\n",
              "sci.fi                 int64\n",
              "sport                  int64\n",
              "musical                int64\n",
              "family                 int64\n",
              "western                int64\n",
              "multigenre             int64\n",
              "word.count             int64\n",
              "gender.terms         float64\n",
              "art.terms            float64\n",
              "critical.terms       float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VppLba7GYMk7"
      },
      "source": [
        "raw_data['content'] = raw_data['content'].astype('string')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OSUXiWXCOe3X",
        "outputId": "56b64cae-1d01-462c-e5d8-8eae760df324"
      },
      "source": [
        "raw_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc.id</th>\n",
              "      <th>content</th>\n",
              "      <th>primaryTitle</th>\n",
              "      <th>lead.actor.feamle</th>\n",
              "      <th>female.director</th>\n",
              "      <th>media.name</th>\n",
              "      <th>year</th>\n",
              "      <th>rating</th>\n",
              "      <th>rating_names</th>\n",
              "      <th>no.rating</th>\n",
              "      <th>pub.type2</th>\n",
              "      <th>newspaper.name</th>\n",
              "      <th>elite.np</th>\n",
              "      <th>author.female</th>\n",
              "      <th>genres</th>\n",
              "      <th>first.genre</th>\n",
              "      <th>crime</th>\n",
              "      <th>drama</th>\n",
              "      <th>action</th>\n",
              "      <th>horror</th>\n",
              "      <th>comedy</th>\n",
              "      <th>mystery</th>\n",
              "      <th>romance</th>\n",
              "      <th>adventure</th>\n",
              "      <th>animation</th>\n",
              "      <th>thriller</th>\n",
              "      <th>fantasy</th>\n",
              "      <th>sci.fi</th>\n",
              "      <th>sport</th>\n",
              "      <th>musical</th>\n",
              "      <th>family</th>\n",
              "      <th>western</th>\n",
              "      <th>multigenre</th>\n",
              "      <th>word.count</th>\n",
              "      <th>gender.terms</th>\n",
              "      <th>art.terms</th>\n",
              "      <th>critical.terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2815</td>\n",
              "      <td>10,000 B.C. is its name. Pre-history is its ga...</td>\n",
              "      <td>10,000 BC</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Houston Chronicle</td>\n",
              "      <td>2008</td>\n",
              "      <td>0.25</td>\n",
              "      <td>Unfavorable</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>Houston Chronicle</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Action,Adventure,Drama</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>600</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3794</td>\n",
              "      <td>10,000 B.C. is the latest epic to sail into th...</td>\n",
              "      <td>10,000 BC</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Houston Chronicle</td>\n",
              "      <td>2008</td>\n",
              "      <td>0.375</td>\n",
              "      <td>Mixed</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>Houston Chronicle</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Action,Adventure,Drama</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>477</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>649</td>\n",
              "      <td>As far as writer-director Roland Emmerich is ...</td>\n",
              "      <td>10,000 BC</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>lat</td>\n",
              "      <td>2008</td>\n",
              "      <td>Favorable</td>\n",
              "      <td>Favorable</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>lat</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Action,Adventure,Drama</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>680</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>635</td>\n",
              "      <td>''Only time can teach us what is truth and wha...</td>\n",
              "      <td>10,000 BC</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>nyt</td>\n",
              "      <td>2008</td>\n",
              "      <td>UnFavorable</td>\n",
              "      <td>Unfavorable</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>nyt</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Action,Adventure,Drama</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>614</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4277</td>\n",
              "      <td>Yabba-dabba-don't. A granite-headed bomb from ...</td>\n",
              "      <td>10,000 BC</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Star-Ledger</td>\n",
              "      <td>2008</td>\n",
              "      <td>0.25</td>\n",
              "      <td>Unfavorable</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>Star-Ledger</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Action,Adventure,Drama</td>\n",
              "      <td>Action</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>561</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262</th>\n",
              "      <td>3357</td>\n",
              "      <td>Your Friends and Neighbors is often a funny mo...</td>\n",
              "      <td>Your Friends &amp; Neighbors</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Houston Chronicle</td>\n",
              "      <td>1998</td>\n",
              "      <td>No Rating</td>\n",
              "      <td>No Rating</td>\n",
              "      <td>1</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>Houston Chronicle</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Comedy,Drama,Romance</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>487</td>\n",
              "      <td>17.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1263</th>\n",
              "      <td>3947</td>\n",
              "      <td>Whoever came up with the old ``sticks and ston...</td>\n",
              "      <td>Your Friends &amp; Neighbors</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Miami Herald</td>\n",
              "      <td>1998</td>\n",
              "      <td>0.875</td>\n",
              "      <td>Favorable</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>Miami Herald</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Comedy,Drama,Romance</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>657</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>3133</td>\n",
              "      <td>The title sequence of \"\"Your Friends and Neigh...</td>\n",
              "      <td>Your Friends &amp; Neighbors</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Star Tribune</td>\n",
              "      <td>1998</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Favorable</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>Star Tribune</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Comedy,Drama,Romance</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>614</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1265</th>\n",
              "      <td>2901</td>\n",
              "      <td>Exotic dancers discover that being undead earn...</td>\n",
              "      <td>Zombie Strippers!</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Houston Chronicle</td>\n",
              "      <td>2008</td>\n",
              "      <td>0.125</td>\n",
              "      <td>Unfavorable</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>Houston Chronicle</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Comedy,Horror,Sci-Fi</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>144</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>737</td>\n",
              "      <td>Those excited by the words \"\"Zombie Strippers\"...</td>\n",
              "      <td>Zombie Strippers!</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>nyt</td>\n",
              "      <td>2008</td>\n",
              "      <td>Mixed</td>\n",
              "      <td>Mixed</td>\n",
              "      <td>0</td>\n",
              "      <td>newspaper</td>\n",
              "      <td>nyt</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Comedy,Horror,Sci-Fi</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>233</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1267 rows Ã— 37 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      doc.id  ... critical.terms\n",
              "0       2815  ...            0.0\n",
              "1       3794  ...            0.0\n",
              "2        649  ...            0.0\n",
              "3        635  ...            0.0\n",
              "4       4277  ...            0.0\n",
              "...      ...  ...            ...\n",
              "1262    3357  ...            0.0\n",
              "1263    3947  ...            3.0\n",
              "1264    3133  ...            0.0\n",
              "1265    2901  ...            1.0\n",
              "1266     737  ...            0.0\n",
              "\n",
              "[1267 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmjBNkFRUW4k"
      },
      "source": [
        "text = []\n",
        "for i in range(0, len(raw_data['content'])):\n",
        "  text.append(raw_data['content'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FNMIpoTOn3S"
      },
      "source": [
        "# Topic Modeling with LDA\n",
        "\n",
        "In this section, we will go through how to use python pacakages (*gensim*) to perform the topic analysis.\n",
        "\n",
        "Reference: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9UFwrb5bGPk"
      },
      "source": [
        "Wrapped-in function, you dont have to know all the details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi1MolwEGi3-"
      },
      "source": [
        "# Importing the needed packages\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "# wrapped function\n",
        "def extract_topic(text, stopwords):\n",
        "  # tokenization\n",
        "  tokenized_text = []\n",
        "  for sentence in text:\n",
        "    tokenized_text.append(word_tokenize(sentence))\n",
        "\n",
        "  punctuations = string.punctuation  + \"*\" + \"/\" + \"\\\\\" + \"_\" + \"-\"\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  filtered_text = []\n",
        "\n",
        "  for sent in tokenized_text:\n",
        "    filtered_list = []\n",
        "    for word in sent:\n",
        "      # filter out tokens that have punctuations and numbers\n",
        "        # word.isalpha() returns true if a string only contains letters.\n",
        "      # filter out stop words\n",
        "      if word.isalpha() and lemmatizer.lemmatize(word.lower()) not in stop_words and len(word) >= 2:\n",
        "        filtered_list.append(lemmatizer.lemmatize(word.lower()))\n",
        "    filtered_text.append(filtered_list)\n",
        "\n",
        "\n",
        "  # Create Dictionary\n",
        "  id2word = corpora.Dictionary(filtered_text)\n",
        "  # Create Corpus\n",
        "  texts = filtered_text\n",
        "\n",
        "  # Coverting Text to Bag of Words features\n",
        "  corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20, \n",
        "                                           random_state= 0,\n",
        "                                           passes = 10,\n",
        "                                           alpha='auto')\n",
        "  \n",
        "\n",
        "  return lda_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMGM7i4DbR24"
      },
      "source": [
        "### Extract Topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1T50AHFHDK8"
      },
      "source": [
        "# filtering stop words (numbers) and punctuations, and lemmatzing\n",
        "stop_words = stopwords.words(\"english\")\n",
        "#stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'line', 'organization', 'university', 'wa', 'ha', \"'s\", \"n't\", \"'d\"])\n",
        "stop_words.extend([ 'film', 'movie', 'films', 'movies', 'like','wa', 'ha', 'one', \"'s\", \"n't\"])\n",
        "\n",
        "\n",
        "lda_model = extract_topic(text, stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ7Nf4MXJraN"
      },
      "source": [
        "## Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qamE_j5SBlg"
      },
      "source": [
        "### Prepareing data for LDA Analysis\n",
        "\n",
        "As the first step, we will follow pre-processing methods leanred previously to pre-process the data like tokenization, filtering out the stop words, lemmatizing, building the word dictionary and etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJAMkGD8Xfb5"
      },
      "source": [
        "# Importing the needed packages\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0xUNRFNSE0p"
      },
      "source": [
        "# tokenization\n",
        "tokenized_text = []\n",
        "for sentence in text:\n",
        "  tokenized_text.append(word_tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpF2-1_IUbse"
      },
      "source": [
        "# filtering stop words (numbers) and punctuations, and lemmatzing\n",
        "stop_words = stopwords.words(\"english\")\n",
        "#stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'line', 'organization', 'university', 'wa', 'ha', \"'s\", \"n't\", \"'d\"])\n",
        "stop_words.extend([ 'film', 'movie', 'films', 'movies', 'like', 'ha', 'one', \"'s\", \"n't\", \"'d\"])\n",
        "\n",
        "\n",
        "punctuations = string.punctuation  + \"*\" + \"/\" + \"\\\\\" + \"_\" + \"-\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "filtered_text = []\n",
        "\n",
        "for sent in tokenized_text:\n",
        "  filtered_list = []\n",
        "  for word in sent:\n",
        "    # filter out tokens that have punctuations and numbers\n",
        "      # word.isalpha() returns true if a string only contains letters.\n",
        "    # filter out stop words\n",
        "    if word.isalpha() and lemmatizer.lemmatize(word.lower()) not in stop_words and len(word) >= 2:\n",
        "      filtered_list.append(lemmatizer.lemmatize(word.lower()))\n",
        "  filtered_text.append(filtered_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7To7pe6vUx_1"
      },
      "source": [
        "### Creating the Dictionary and Corpus needed for LDA Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8fwAC-hU4C7"
      },
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(filtered_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pqrr19PkVM_a",
        "outputId": "30b07804-6594-4259-ea03-3fdb96029241"
      },
      "source": [
        "id2word[0] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'acting'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qHfkhchVNfr"
      },
      "source": [
        "# Create Corpus\n",
        "texts = filtered_text\n",
        "\n",
        "# Coverting Text to Bag of Words features\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xD85cAlVGmy",
        "outputId": "4b24c113-bb43-418b-d45c-649f1f4070ef"
      },
      "source": [
        "corpus[0]\n",
        "# (word_id, word_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1),\n",
              " (1, 1),\n",
              " (2, 1),\n",
              " (3, 1),\n",
              " (4, 1),\n",
              " (5, 1),\n",
              " (6, 3),\n",
              " (7, 2),\n",
              " (8, 1),\n",
              " (9, 1),\n",
              " (10, 1),\n",
              " (11, 1),\n",
              " (12, 1),\n",
              " (13, 1),\n",
              " (14, 1),\n",
              " (15, 1),\n",
              " (16, 1),\n",
              " (17, 2),\n",
              " (18, 1),\n",
              " (19, 1),\n",
              " (20, 1),\n",
              " (21, 1),\n",
              " (22, 1),\n",
              " (23, 1),\n",
              " (24, 2),\n",
              " (25, 1),\n",
              " (26, 1),\n",
              " (27, 1),\n",
              " (28, 1),\n",
              " (29, 1),\n",
              " (30, 1),\n",
              " (31, 1),\n",
              " (32, 1),\n",
              " (33, 1),\n",
              " (34, 1),\n",
              " (35, 1),\n",
              " (36, 1),\n",
              " (37, 1),\n",
              " (38, 1),\n",
              " (39, 2),\n",
              " (40, 1),\n",
              " (41, 1),\n",
              " (42, 2),\n",
              " (43, 1),\n",
              " (44, 2),\n",
              " (45, 1),\n",
              " (46, 1),\n",
              " (47, 1),\n",
              " (48, 1),\n",
              " (49, 1),\n",
              " (50, 1),\n",
              " (51, 1),\n",
              " (52, 1),\n",
              " (53, 1),\n",
              " (54, 1),\n",
              " (55, 1),\n",
              " (56, 1),\n",
              " (57, 1),\n",
              " (58, 1),\n",
              " (59, 1),\n",
              " (60, 1),\n",
              " (61, 1),\n",
              " (62, 1),\n",
              " (63, 2),\n",
              " (64, 2),\n",
              " (65, 1),\n",
              " (66, 1),\n",
              " (67, 1),\n",
              " (68, 1),\n",
              " (69, 1),\n",
              " (70, 1),\n",
              " (71, 1),\n",
              " (72, 1),\n",
              " (73, 1),\n",
              " (74, 1),\n",
              " (75, 1),\n",
              " (76, 1),\n",
              " (77, 1),\n",
              " (78, 1),\n",
              " (79, 1),\n",
              " (80, 2),\n",
              " (81, 1),\n",
              " (82, 1),\n",
              " (83, 1),\n",
              " (84, 1),\n",
              " (85, 1),\n",
              " (86, 1),\n",
              " (87, 1),\n",
              " (88, 1),\n",
              " (89, 1),\n",
              " (90, 1),\n",
              " (91, 1),\n",
              " (92, 1),\n",
              " (93, 1),\n",
              " (94, 1),\n",
              " (95, 1),\n",
              " (96, 1),\n",
              " (97, 1),\n",
              " (98, 1),\n",
              " (99, 2),\n",
              " (100, 1),\n",
              " (101, 1),\n",
              " (102, 1),\n",
              " (103, 2),\n",
              " (104, 1),\n",
              " (105, 1),\n",
              " (106, 1),\n",
              " (107, 4),\n",
              " (108, 1),\n",
              " (109, 1),\n",
              " (110, 1),\n",
              " (111, 1),\n",
              " (112, 1),\n",
              " (113, 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDJQG8K4VK8g",
        "outputId": "22b80ea8-8475-4cec-cec6-9db00a4c2a9f"
      },
      "source": [
        "print(id2word[0])\n",
        "id2word[0] in filtered_text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azI5HQkmVZyP",
        "outputId": "52bf9e95-8d8c-472d-e2b5-70297d81d5a8"
      },
      "source": [
        "print(id2word[60])\n",
        "id2word[57] in filtered_text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "legal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd5j_d17WDF2",
        "outputId": "9dd2f692-cea7-4a29-a12d-fe5408a7a9df"
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('acting', 1),\n",
              "  ('allure', 1),\n",
              "  ('always', 1),\n",
              "  ('antic', 1),\n",
              "  ('attests', 1),\n",
              "  ('beat', 1),\n",
              "  ('ben', 3),\n",
              "  ('card', 2),\n",
              "  ('cash', 1),\n",
              "  ('casino', 1),\n",
              "  ('catch', 1),\n",
              "  ('chop', 1),\n",
              "  ('clichÃ©s', 1),\n",
              "  ('cocktail', 1),\n",
              "  ('come', 1),\n",
              "  ('convention', 1),\n",
              "  ('could', 1),\n",
              "  ('counting', 2),\n",
              "  ('court', 1),\n",
              "  ('decide', 1),\n",
              "  ('desert', 1),\n",
              "  ('despite', 1),\n",
              "  ('dictate', 1),\n",
              "  ('direction', 1),\n",
              "  ('doe', 2),\n",
              "  ('drag', 1),\n",
              "  ('earn', 1),\n",
              "  ('either', 1),\n",
              "  ('elusive', 1),\n",
              "  ('endeavor', 1),\n",
              "  ('enough', 1),\n",
              "  ('entertaining', 1),\n",
              "  ('even', 1),\n",
              "  ('fishburne', 1),\n",
              "  ('fortune', 1),\n",
              "  ('fun', 1),\n",
              "  ('gap', 1),\n",
              "  ('gathered', 1),\n",
              "  ('gent', 1),\n",
              "  ('glamour', 2),\n",
              "  ('glitz', 1),\n",
              "  ('glitzy', 1),\n",
              "  ('go', 2),\n",
              "  ('god', 1),\n",
              "  ('hard', 2),\n",
              "  ('harvard', 1),\n",
              "  ('head', 1),\n",
              "  ('held', 1),\n",
              "  ('jim', 1),\n",
              "  ('join', 1),\n",
              "  ('kevin', 1),\n",
              "  ('kid', 1),\n",
              "  ('knavish', 1),\n",
              "  ('knock', 1),\n",
              "  ('knocking', 1),\n",
              "  ('la', 1),\n",
              "  ('lady', 1),\n",
              "  ('laurence', 1),\n",
              "  ('leaf', 1),\n",
              "  ('leave', 1),\n",
              "  ('legal', 1),\n",
              "  ('lighter', 1),\n",
              "  ('logic', 1),\n",
              "  ('luck', 2),\n",
              "  ('make', 2),\n",
              "  ('many', 1),\n",
              "  ('merely', 1),\n",
              "  ('mix', 1),\n",
              "  ('must', 1),\n",
              "  ('obvious', 1),\n",
              "  ('paced', 1),\n",
              "  ('path', 1),\n",
              "  ('perfectly', 1),\n",
              "  ('place', 1),\n",
              "  ('played', 1),\n",
              "  ('plenty', 1),\n",
              "  ('plucky', 1),\n",
              "  ('potential', 1),\n",
              "  ('predictable', 1),\n",
              "  ('predictably', 1),\n",
              "  ('professor', 2),\n",
              "  ('program', 1),\n",
              "  ('prove', 1),\n",
              "  ('pull', 1),\n",
              "  ('raise', 1),\n",
              "  ('rake', 1),\n",
              "  ('reliance', 1),\n",
              "  ('scholarship', 1),\n",
              "  ('security', 1),\n",
              "  ('seduces', 1),\n",
              "  ('seediness', 1),\n",
              "  ('seems', 1),\n",
              "  ('sense', 1),\n",
              "  ('showy', 1),\n",
              "  ('spacey', 1),\n",
              "  ('still', 1),\n",
              "  ('story', 1),\n",
              "  ('strange', 1),\n",
              "  ('student', 1),\n",
              "  ('sturgess', 2),\n",
              "  ('system', 1),\n",
              "  ('team', 1),\n",
              "  ('teeth', 1),\n",
              "  ('though', 2),\n",
              "  ('transformation', 1),\n",
              "  ('tuition', 1),\n",
              "  ('unpunished', 1),\n",
              "  ('vega', 4),\n",
              "  ('wallet', 1),\n",
              "  ('well', 1),\n",
              "  ('work', 1),\n",
              "  ('worked', 1),\n",
              "  ('yet', 1),\n",
              "  ('zippy', 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVnPb5rhWgrt"
      },
      "source": [
        "### Building the LDA Model\n",
        "\n",
        "In this section we will learn the LDA model thourgh the LDA module in gensim. (https://radimrehurek.com/gensim/models/ldamodel.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZiGinh6WkIx"
      },
      "source": [
        "import gensim\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20, \n",
        "                                           random_state= 0,\n",
        "                                           passes = 10,\n",
        "                                           alpha='auto')\n",
        "# for more details, please refers to: https://radimrehurek.com/gensim/models/ldamodel.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPWjMc4pXsSe"
      },
      "source": [
        "## View the topics in LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U--9QvBXvXN",
        "outputId": "447038a4-a91a-446d-a86f-82cf3e118513"
      },
      "source": [
        "lda_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.021*\"christopher\" + 0.015*\"robin\" + 0.013*\"pooh\" + 0.007*\"prefontaine\" + 0.007*\"pacino\" + 0.006*\"luke\" + 0.005*\"wood\" + 0.005*\"find\" + 0.005*\"work\" + 0.004*\"mcgregor\"'),\n",
              " (1,\n",
              "  '0.007*\"truman\" + 0.006*\"earl\" + 0.005*\"larry\" + 0.004*\"love\" + 0.004*\"eastwood\" + 0.003*\"myers\" + 0.003*\"guru\" + 0.003*\"two\" + 0.003*\"frank\" + 0.003*\"life\"'),\n",
              " (2,\n",
              "  '0.012*\"hancock\" + 0.008*\"david\" + 0.008*\"nic\" + 0.007*\"addiction\" + 0.006*\"slade\" + 0.005*\"carell\" + 0.005*\"father\" + 0.005*\"felix\" + 0.005*\"chalamet\" + 0.005*\"drug\"'),\n",
              " (3,\n",
              "  '0.022*\"mulder\" + 0.019*\"moses\" + 0.018*\"scully\" + 0.010*\"egypt\" + 0.010*\"alien\" + 0.009*\"anderson\" + 0.008*\"prince\" + 0.008*\"fbi\" + 0.008*\"truth\" + 0.006*\"duchovny\"'),\n",
              " (4,\n",
              "  '0.010*\"hank\" + 0.010*\"joe\" + 0.010*\"ryan\" + 0.009*\"kathleen\" + 0.009*\"mail\" + 0.008*\"got\" + 0.007*\"meg\" + 0.007*\"grrltalk\" + 0.006*\"movieman\" + 0.005*\"book\"'),\n",
              " (5,\n",
              "  '0.006*\"marx\" + 0.006*\"chapman\" + 0.005*\"young\" + 0.004*\"tucci\" + 0.004*\"tolyan\" + 0.004*\"lennon\" + 0.004*\"katya\" + 0.003*\"also\" + 0.003*\"sanya\" + 0.003*\"thief\"'),\n",
              " (6,\n",
              "  '0.012*\"milk\" + 0.006*\"harvey\" + 0.006*\"gay\" + 0.006*\"president\" + 0.005*\"white\" + 0.005*\"de\" + 0.004*\"american\" + 0.004*\"van\" + 0.004*\"cruise\" + 0.004*\"war\"'),\n",
              " (7,\n",
              "  '0.007*\"kevin\" + 0.007*\"sam\" + 0.006*\"woop\" + 0.006*\"band\" + 0.005*\"fish\" + 0.004*\"mighty\" + 0.004*\"max\" + 0.004*\"rocker\" + 0.003*\"stone\" + 0.003*\"teddy\"'),\n",
              " (8,\n",
              "  '0.005*\"character\" + 0.004*\"make\" + 0.004*\"time\" + 0.004*\"even\" + 0.004*\"story\" + 0.003*\"star\" + 0.003*\"first\" + 0.003*\"doe\" + 0.003*\"well\" + 0.003*\"much\"'),\n",
              " (9,\n",
              "  '0.004*\"go\" + 0.004*\"get\" + 0.004*\"show\" + 0.003*\"much\" + 0.003*\"even\" + 0.003*\"make\" + 0.003*\"also\" + 0.003*\"story\" + 0.003*\"time\" + 0.003*\"take\"'),\n",
              " (10,\n",
              "  '0.014*\"zorro\" + 0.011*\"mehdi\" + 0.011*\"manu\" + 0.009*\"sarah\" + 0.008*\"adrien\" + 0.007*\"diego\" + 0.006*\"alejandro\" + 0.006*\"bening\" + 0.006*\"witness\" + 0.006*\"banderas\"'),\n",
              " (11,\n",
              "  '0.006*\"make\" + 0.005*\"character\" + 0.005*\"comedy\" + 0.005*\"doe\" + 0.005*\"get\" + 0.004*\"even\" + 0.003*\"time\" + 0.003*\"way\" + 0.003*\"star\" + 0.003*\"thing\"'),\n",
              " (12,\n",
              "  '0.028*\"walter\" + 0.027*\"tarek\" + 0.019*\"billy\" + 0.010*\"gallo\" + 0.009*\"zainab\" + 0.009*\"visitor\" + 0.008*\"buffalo\" + 0.008*\"layla\" + 0.006*\"vale\" + 0.006*\"hillinger\"'),\n",
              " (13,\n",
              "  '0.019*\"mauro\" + 0.009*\"parent\" + 0.008*\"sebastian\" + 0.007*\"vacation\" + 0.007*\"grandfather\" + 0.006*\"shlomo\" + 0.006*\"william\" + 0.006*\"soccer\" + 0.006*\"juan\" + 0.005*\"went\"'),\n",
              " (14,\n",
              "  '0.008*\"zohan\" + 0.007*\"miller\" + 0.006*\"rick\" + 0.006*\"spirit\" + 0.005*\"eleanor\" + 0.004*\"actor\" + 0.004*\"sandler\" + 0.004*\"arthur\" + 0.004*\"director\" + 0.003*\"hurt\"'),\n",
              " (15,\n",
              "  '0.005*\"doe\" + 0.005*\"life\" + 0.005*\"story\" + 0.005*\"character\" + 0.004*\"much\" + 0.004*\"man\" + 0.004*\"time\" + 0.004*\"actor\" + 0.003*\"make\" + 0.003*\"even\"'),\n",
              " (16,\n",
              "  '0.026*\"kennedy\" + 0.021*\"ted\" + 0.010*\"chappaquiddick\" + 0.009*\"mary\" + 0.008*\"ramsay\" + 0.008*\"jo\" + 0.008*\"van\" + 0.008*\"misumi\" + 0.006*\"shigemori\" + 0.006*\"clarke\"'),\n",
              " (17,\n",
              "  '0.025*\"wilde\" + 0.007*\"love\" + 0.007*\"oscar\" + 0.006*\"bosie\" + 0.006*\"oath\" + 0.006*\"fry\" + 0.005*\"celadon\" + 0.004*\"victorian\" + 0.004*\"barinholtz\" + 0.004*\"astrea\"'),\n",
              " (18,\n",
              "  '0.007*\"character\" + 0.007*\"stiller\" + 0.007*\"thunder\" + 0.006*\"tropic\" + 0.005*\"actor\" + 0.004*\"knight\" + 0.004*\"speedman\" + 0.003*\"dark\" + 0.003*\"batman\" + 0.003*\"lazarus\"'),\n",
              " (19,\n",
              "  '0.005*\"story\" + 0.004*\"time\" + 0.004*\"make\" + 0.004*\"even\" + 0.004*\"character\" + 0.003*\"also\" + 0.003*\"first\" + 0.003*\"much\" + 0.003*\"doe\" + 0.003*\"life\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5osdPwa7YEMd"
      },
      "source": [
        "### How to interpret the extracted topics?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6cV5c4NYpj4"
      },
      "source": [
        "For example, Topic 0 is represented as \n",
        "\n",
        "```\n",
        "0.033*\"god\" + 0.014*\"christian\" + 0.013*\"jesus\" + 0.009*\"bible\" + 0.008*\"one\" + 0.007*\"church\" + 0.006*\"christ\" + 0.006*\"faith\" + 0.005*\"say\" + 0.005*\"religion\"\n",
        "```\n",
        "\n",
        "It means the top 10 keywords that contribute to this topic are: god, christian, jesus, bible and etc. And the weight of food on topic 0 is 0.033.\n",
        "\n",
        "Looking at these keywords, can you guess what this topic could be? You may summarise it either are 'religion' or 'christian.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOnLIkbsYBS7"
      },
      "source": [
        "# Getting labels for all the news\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE5KHoRJdf81",
        "outputId": "769d3c7a-3e52-4fc5-8f1b-0e123d2dbcfb"
      },
      "source": [
        "print(text[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10,000 B.C. is its name. Pre-history is its game. But what I want to know is: Where? On which planet does it take place? Because the last time I checked, pyramids weren't built during the Pleistocene. And they weren't exactly walking distance from the last snowy remnants of a diminishing ice age. But what a kvetch I am. So what if this colossally hokey saga mashes up history and geography and climatology and anthropology? So what if it links up land masses that have been unconnected since Pangea broke apart 180 million years ago? Who am I to complain, McGraw-Hill? Publicity materials quote Harald Kloser (who executive produced, co-wrote and co-scored the film) saying he and director/writer Roland Emmerich ``never intended for 10,000 B.C. to be a documentary,'' which is a huge relief. They also quote Emmerich on the little matter of location, helpfully pinpointing the setting as ``our own made-up Africa.'' In filmmaking terms, this means portions were shot in New Zealand (for snowy mountains), South Africa (for the jungle below) and Namibia (for the desert nearby). In narrative terms, it means that African warrior tribes can unite in a jiffy with light-skinned hunters in funky dreads to fight off kind-of Persian, sort-of Egyptian slave-masters building vaguely pre-Columbian or possibly Pharaonic ziggurats assisted by wooly mammoths. The plot, such as it is, follows our hero D'leh (pronounced ``Delay,'' as in postponement or Tom) and three brave comrades as they race to save loved ones kidnapped by these whip-wielding demons on horseback. The loveliest of the loved ones is D'leh's beloved, Evolet, who has blue eyes and a jaw-dropping ability to do absolutely nothing except stand around with her mouth open until the time comes for her to weep, at which point her perfectly applied eyeliner begins to run. Fashionistas and followers of international starlets will be pleased to learn that Evolet is played by Camilla Belle, the terror-stricken baby-sitter in the 2006 remake of \"\"When A Stranger Calls.\"\" Steven Strait, who played \"\"Sky High's\"\" mutant flamethrower, wields a long spear and a giant wig as D'Leh, while the fine kiwi actor Cliff Curtis (\"\"Whale Rider,\"\" \"\"Once Were Warriors\"\") heroically maintains his dignity as D'Leh's rival and mentor, Tic'Tic. And if you can't quite place the crinkly narrator, that's the voice Omar Sharif. It's Dr. Zhivago's job to warn us that \"\"some truths do not survive the ages,\"\" and I'm assuming he means that until now, no one knew that hunter-gatherers wore Maybelline. Visually, 10,000 B.C. boasts the oohs and ahhs of dusky primordial landscapes, huffing slave ships and big-budget CG creatures. The mammoths are cool. The squealing killer ostriches, perhaps inspired by 70 million-year-old Gigantoraptor fossils, are idiotic but ... OK, they're idiotic. And the saber-toothed tiger isn't so bad, though it looks about as real as \"\"Diego in Ice Age 2: The Meltdown. \"\" But all the glam effects in the world add up to nil in a corny, derivative and incoherent script that dishes out cheese-fried Apocalypto with a dash of 300. There is fun to be had, however unintentional, when D'Leh halts an advancing army by screaming \"\"Stop! Stop!\"\" Or when he tells his tribesmen, \"\"I keeled thee (mammoth). I claim thee white spear, and weeth it my woman, Evolet.\"\" (Tribesmen: \"\"Yah! Yah! Yah!\"\") And it's worth noting that Emmerich's last film was \"\"The Day After Tomorrow,\"\" the eco-disaster flick that foretold sudden glaciation; until I saw it, I didn't know that you could outrun frost. Here's hoping he won't make yet another movie about yet another Ice Age - for at least another 10,000 years.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3GhLnc0kRAD",
        "outputId": "57589e99-5030-41d4-c0de-60f7e2cd23a9"
      },
      "source": [
        "doc_lda[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2, 0.8742272), (5, 0.12323051)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE1LQKQFkSwa",
        "outputId": "dc7dadc7-2844-4a4e-beee-860fe43dacf9"
      },
      "source": [
        "lda_model.print_topics()[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17,\n",
              " '0.017*\"skate\" + 0.014*\"camille\" + 0.011*\"kitchen\" + 0.009*\"girl\" + 0.007*\"new\" + 0.007*\"moselle\" + 0.006*\"city\" + 0.005*\"skater\" + 0.005*\"skating\" + 0.005*\"york\"')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjThuCwQtH3U",
        "outputId": "1531e3f9-fbff-4dd3-861b-2c94e6fde9a8"
      },
      "source": [
        "print(text[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10,000 B.C. is the latest epic to sail into theaters, telling the courageous, prehistoric story of how far one man will go to save the woman he loves, bring down an evil tyrant and toss his locks manfully at the screen. Wait - it's supposed to take place on Earth? One might find it far easier to view 10,000 B.C. as taking place in a parallel dimension, one in which wooly mammoths act as oxen, New Zealand borders the Amazon and Bob Marley hair is the height of fashion. Unfortunately, its one similarity with Earth is that it has the same clichÃ©s. Our hero, D'leh (Steven Strait), wants to rise above his self-doubt and daddy issues. When raiders sack his village and carry off his love interest, Evolet (Camilla Belle), it seems like a great opportunity to do so. We know D'leh is our hero because a windbaggy narrator tells us. Every hero must have his Obi-Wan Kenobi, and D'leh is no different. He's the mentee to the tribe's greatest hunter, Tic'Tic (Cliff Curtis), who leaves the defenseless village to join D'leh's rescue effort. Pulling together a few men whom the raiders missed, the group sets off to reclaim its stolen comrades. D'leh soon realizes that to save his love, he must bring down a slave-labor-driven, theocratic civilization. Luckily, a few Zulu warriors offer to help. In a nutshell, 10,000 B.C. is a mashup of Apocalypto, Spartacus, One Million Years B.C.'s Raquel Welch and The Flintstones. The actual film makes little more sense. 10,000 B.C. not only resurrects extinct species and creates fictional Egyptian-Aztec-ish civilizations but does so with no clear logic. D'leh and Evolet have dreadlocks and dirt-stained faces yet sport bleached teeth and mascara. A trek from frigid European mountains to the steamy rain forest to the African desert takes about a week. The rest of the story merely recaps a thousand other ye-olde-hero journeys. Perhaps it's trying to establish the original archetypes - the people who started them. But it goes about it in such a bland way that no one cares. D'leh lacks charisma, Evolet does nothing, and only Tic'Tic seems capable of any spear-stabbing action. The flesh-eating giant predatory birds have far more presence. The evil civilization sparks a bit more interest: A pasty overlord and his flashy groupies rule over it. The one potentially complex character is the nameless warlord (Affif Ben Badra) who leads the attack on D'leh's village and quickly falls for Evolet. But since he's neither hot nor Bob Marley-coiffed, he's just one more villain in need of stabbing. But 10,000 B.C. manages to outdo other recent duds such as In the Name of the King. With few action sequences, such as a wooly mammoth stampede, and unintentional hilarity courtesy of a poorly written script, the film has popcorn entertainment value. It is entertaining but far from engrossing.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSyc5nqhtLcE",
        "outputId": "56ca6089-8e82-4bf4-e1f6-4c3e01190646"
      },
      "source": [
        "doc_lda[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(17, 0.99666184)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU-fVmNFtM7V",
        "outputId": "1e2b1574-652a-4657-de9c-37c34c2f8884"
      },
      "source": [
        "lda_model.print_topics()[6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,\n",
              " '0.008*\"stella\" + 0.005*\"woman\" + 0.005*\"erin\" + 0.004*\"doe\" + 0.004*\"also\" + 0.004*\"character\" + 0.004*\"make\" + 0.003*\"alan\" + 0.003*\"get\" + 0.003*\"time\"')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgA3PBsebSSm"
      },
      "source": [
        "### Practice: \n",
        "What about the other topics? Can you try to summariza the topics based on these key words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBscE3f-lW2u"
      },
      "source": [
        "This is where I try to answer the Practice question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg6B6ItLlWXV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yESfNr_EydvY"
      },
      "source": [
        "# Word Embeddings\n",
        "\n",
        "In this section, we will learn how to use pre-trained embeddings from gensim to represent words with vectors.\n",
        "\n",
        "Live demo: http://bionlp-www.utu.fi/wv_demo/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPkyePM2y1i-"
      },
      "source": [
        "### Load Pre-trained Word Embeddings from Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyhH5zXAyfyD"
      },
      "source": [
        "import numpy as np\n",
        "import gensim.downloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS2cFnDcyZN4",
        "outputId": "db4c9cc9-531c-4bb0-eaa5-de33471528f9"
      },
      "source": [
        "# View all types of pre-trained embeddings \n",
        "list(gensim.downloader.info()['models'].keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fasttext-wiki-news-subwords-300',\n",
              " 'conceptnet-numberbatch-17-06-300',\n",
              " 'word2vec-ruscorpora-300',\n",
              " 'word2vec-google-news-300',\n",
              " 'glove-wiki-gigaword-50',\n",
              " 'glove-wiki-gigaword-100',\n",
              " 'glove-wiki-gigaword-200',\n",
              " 'glove-wiki-gigaword-300',\n",
              " 'glove-twitter-25',\n",
              " 'glove-twitter-50',\n",
              " 'glove-twitter-100',\n",
              " 'glove-twitter-200',\n",
              " '__testing_word2vec-matrix-synopsis']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXIFlOLsybBm"
      },
      "source": [
        "# download and load the glove-wiki-gigaword-50\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JaJ10xvN-Gy"
      },
      "source": [
        "### How to Use the Pre-trained Word Embeddings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKXRgXAJOEg8"
      },
      "source": [
        "Retrieve the embedding for a specific word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLumJJlmOJCl",
        "outputId": "91346615-40e2-499a-d493-4a7ac1c88c3b"
      },
      "source": [
        "glove_vectors['atlanta']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.0255  ,  1.14    ,  0.27088 ,  1.2964  , -0.22467 , -0.55808 ,\n",
              "       -1.9727  , -0.52942 ,  0.55607 , -0.48596 , -0.7555  , -0.55799 ,\n",
              "       -0.99334 ,  0.13091 ,  0.83527 , -0.058354, -0.79702 , -0.5973  ,\n",
              "       -0.43055 ,  0.095148, -0.42831 ,  0.5277  , -0.41006 ,  0.64514 ,\n",
              "       -0.59836 , -1.0417  , -0.060947, -0.45935 ,  0.79238 , -0.80473 ,\n",
              "        1.781   ,  0.52496 ,  0.036867, -0.51445 , -0.19282 , -0.31396 ,\n",
              "        0.094393,  0.17953 ,  0.91322 ,  0.48565 , -0.053946,  0.3348  ,\n",
              "        0.24868 ,  0.71448 ,  0.040415,  1.4561  , -0.15356 , -0.15673 ,\n",
              "       -0.55824 ,  1.0741  ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfYQezVCOK54",
        "outputId": "d2c8f1df-8860-4c18-87a5-978acf965367"
      },
      "source": [
        "glove_vectors['atlanta'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YK5R5sdOXtq"
      },
      "source": [
        "Find the most similar words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qz1nOQbzJSD",
        "outputId": "2ef03ec4-82e9-4f4c-a560-258cfc8e456d"
      },
      "source": [
        "glove_vectors.most_similar('atlanta')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('denver', 0.8360953330993652),\n",
              " ('houston', 0.833003044128418),\n",
              " ('dallas', 0.8196703195571899),\n",
              " ('seattle', 0.8138191103935242),\n",
              " ('austin', 0.808619499206543),\n",
              " ('miami', 0.8046270608901978),\n",
              " ('tampa', 0.7782285213470459),\n",
              " ('angeles', 0.7770278453826904),\n",
              " ('cincinnati', 0.7677363753318787),\n",
              " ('phoenix', 0.7639263868331909)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ-66fBSz4yh",
        "outputId": "2d53151a-067f-4d60-df00-ed10b388d7a3"
      },
      "source": [
        "glove_vectors.most_similar('table')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tables', 0.8177436590194702),\n",
              " ('place', 0.7994711399078369),\n",
              " ('sit', 0.7571904063224792),\n",
              " ('set', 0.7330495119094849),\n",
              " ('open', 0.7316288948059082),\n",
              " ('hold', 0.730669379234314),\n",
              " ('here', 0.7287872433662415),\n",
              " ('each', 0.7207204699516296),\n",
              " ('bottom', 0.7166720628738403),\n",
              " ('top', 0.7153379917144775)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4g3HM9RL2EU"
      },
      "source": [
        "### Embeddings Capture Relational Meaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmZDU7CxLGqr"
      },
      "source": [
        "# calculate the cosine similarity between two vectors\n",
        "def cosine_sim(a,b):\n",
        "  return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K0CeQytL0Jt",
        "outputId": "a3308d03-6914-4cf8-efff-813ea4e2cbff"
      },
      "source": [
        "cosine_sim(glove_vectors['king'],glove_vectors['queen'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7839044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNmvAP2h0qsP"
      },
      "source": [
        "a = glove_vectors['king'] - glove_vectors['man'] + glove_vectors['woman']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNA0Asz2LYsS",
        "outputId": "3b571478-dc7c-4f2b-f0d8-f603c847a276"
      },
      "source": [
        "cosine_sim(a,glove_vectors['queen'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.86095816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPyBqW2rMZFW"
      },
      "source": [
        "Could you try another example shown in the lecture?\n",
        "\n",
        "e.g., `vector(â€˜parisâ€™) - vector(â€˜franceâ€™) + vector(â€˜italyâ€™)` and `vector(â€˜romeâ€™)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkxcQa3ds5V1",
        "outputId": "242a7b19-76f0-4869-bb45-143bf101a8b1"
      },
      "source": [
        "cosine_sim(glove_vectors['jazz'],glove_vectors['blues'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8367231"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG04LGmlthXk"
      },
      "source": [
        "a = glove_vectors['jazz'] - glove_vectors['rap'] + glove_vectors['rock']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3ZwlZaztj7o",
        "outputId": "272aade7-5c28-410a-90f4-ee5a74f5c541"
      },
      "source": [
        "cosine_sim(a,glove_vectors['pop'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.64199394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8NGLzcBQDCs"
      },
      "source": [
        "# (Optional) Sentence/Document Clustering with Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdI4UR9EQKS_"
      },
      "source": [
        "Now that we know how to use pre-trained word embeddings, let's utilize the embeddings to cluster documents in 20-newsgroups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqem4fV0QXPB"
      },
      "source": [
        "## Prepareing Data\n",
        "\n",
        "Similarly, the first step is to pre-process the input data: tokenization, lammatization and etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FBT2et9QIkO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d478754a-bf73-4d2f-d44a-03757b5e712a"
      },
      "source": [
        "# Importing the needed packages\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# tokenization\n",
        "tokenized_text = []\n",
        "for sentence in text:\n",
        "  tokenized_text.append(word_tokenize(sentence))\n",
        "\n",
        "# filtering stop words (numbers) and punctuations, and lemmatzing\n",
        "stop_words = stopwords.words(\"english\")\n",
        "#stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'line', 'organization', 'university', 'wa', 'ha', \"'s\", \"n't\", \"'d\"])\n",
        "\n",
        "punctuations = string.punctuation  + \"*\" + \"/\" + \"\\\\\" + \"_\" + \"-\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "filtered_text = []\n",
        "\n",
        "for sent in tokenized_text:\n",
        "  filtered_list = []\n",
        "  for word in sent:\n",
        "    # filter out tokens that have punctuations and numbers\n",
        "      # word.isalpha() returns true if a string only contains letters.\n",
        "    # filter out stop words\n",
        "    if word.isalpha() and lemmatizer.lemmatize(word.lower()) not in stop_words and len(word) >= 2:\n",
        "      filtered_list.append(lemmatizer.lemmatize(word.lower()))\n",
        "  filtered_text.append(filtered_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-b2c2e53b4694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# filtering stop words (numbers) and punctuations, and lemmatzing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n4AZQF7QniZ",
        "outputId": "77ca7bce-be5d-450c-ed32-089dd67b3c40"
      },
      "source": [
        "filtered_text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['la',\n",
              " 'vega',\n",
              " 'always',\n",
              " 'held',\n",
              " 'strange',\n",
              " 'allure',\n",
              " 'glamour',\n",
              " 'seediness',\n",
              " 'mix',\n",
              " 'glitzy',\n",
              " 'cocktail',\n",
              " 'many',\n",
              " 'head',\n",
              " 'desert',\n",
              " 'court',\n",
              " 'lady',\n",
              " 'luck',\n",
              " 'leave',\n",
              " 'lighter',\n",
              " 'wallet',\n",
              " 'kid',\n",
              " 'decide',\n",
              " 'make',\n",
              " 'luck',\n",
              " 'beat',\n",
              " 'system',\n",
              " 'plucky',\n",
              " 'endeavor',\n",
              " 'though',\n",
              " 'reliance',\n",
              " 'convention',\n",
              " 'drag',\n",
              " 'ben',\n",
              " 'jim',\n",
              " 'sturgess',\n",
              " 'worked',\n",
              " 'hard',\n",
              " 'earn',\n",
              " 'place',\n",
              " 'harvard',\n",
              " 'program',\n",
              " 'hard',\n",
              " 'work',\n",
              " 'enough',\n",
              " 'raise',\n",
              " 'tuition',\n",
              " 'scholarship',\n",
              " 'seems',\n",
              " 'elusive',\n",
              " 'fortune',\n",
              " 'come',\n",
              " 'knocking',\n",
              " 'knavish',\n",
              " 'professor',\n",
              " 'kevin',\n",
              " 'spacey',\n",
              " 'gathered',\n",
              " 'team',\n",
              " 'student',\n",
              " 'go',\n",
              " 'vega',\n",
              " 'rake',\n",
              " 'cash',\n",
              " 'counting',\n",
              " 'card',\n",
              " 'ben',\n",
              " 'join',\n",
              " 'professor',\n",
              " 'attests',\n",
              " 'card',\n",
              " 'counting',\n",
              " 'perfectly',\n",
              " 'legal',\n",
              " 'merely',\n",
              " 'leaf',\n",
              " 'casino',\n",
              " 'security',\n",
              " 'gent',\n",
              " 'played',\n",
              " 'laurence',\n",
              " 'fishburne',\n",
              " 'knock',\n",
              " 'teeth',\n",
              " 'catch',\n",
              " 'story',\n",
              " 'well',\n",
              " 'paced',\n",
              " 'gap',\n",
              " 'logic',\n",
              " 'doe',\n",
              " 'make',\n",
              " 'sense',\n",
              " 'predictable',\n",
              " 'predictably',\n",
              " 'glamour',\n",
              " 'vega',\n",
              " 'seduces',\n",
              " 'ben',\n",
              " 'antic',\n",
              " 'must',\n",
              " 'go',\n",
              " 'unpunished',\n",
              " 'god',\n",
              " 'clichÃ©s',\n",
              " 'dictate',\n",
              " 'sturgess',\n",
              " 'doe',\n",
              " 'yet',\n",
              " 'acting',\n",
              " 'chop',\n",
              " 'pull',\n",
              " 'transformation',\n",
              " 'either',\n",
              " 'even',\n",
              " 'though',\n",
              " 'plenty',\n",
              " 'potential',\n",
              " 'still',\n",
              " 'showy',\n",
              " 'fun',\n",
              " 'despite',\n",
              " 'obvious',\n",
              " 'path',\n",
              " 'zippy',\n",
              " 'direction',\n",
              " 'vega',\n",
              " 'glitz',\n",
              " 'prove',\n",
              " 'entertaining',\n",
              " 'could']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SYspQnxQ3up"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZYP-tNMQ86i"
      },
      "source": [
        "## Generating Vector Representations of Sentences/Documents\n",
        "\n",
        "A common approach to vectorize a sentence/document is to use the average of all the vectors of words in the sentence/document.\n",
        "\n",
        "For words that are in the pre-trained word embedding models, we directly use the pre-trained embeddings. For words that are not in the pre-trained models, we call them **unknow words**. Usually, we will use a *zero vector* to represent them or just ignore them. If you want a more precise model that could cover those cases, you could train your own word2vec model over your own corpus. For more details, please refer to https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html and  https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_COk0TdKREHa"
      },
      "source": [
        "def vectorize(doc, model):\n",
        "  vectors = []\n",
        "  \n",
        "  # transform every token in the input doc to vectors\n",
        "  for token in doc:\n",
        "    #zero_vector = np.zeros(model.vector_size)\n",
        "    if token in model:\n",
        "      vectors.append(model[token])\n",
        "    #else:\n",
        "    #  vectors.append(zero_vector)\n",
        "\n",
        "  # average word vectors in one document\n",
        "  vectors = np.asarray(vectors)\n",
        "  avg_vec = vectors.mean(axis=0)\n",
        "\n",
        "  return avg_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6UtCAdESHSB",
        "outputId": "bcebecf8-ae96-4f16-ac19-91f818c7b408"
      },
      "source": [
        "vectorize(filtered_text[0], glove_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.4386287e-01,  2.1803944e-01, -2.5903392e-01, -6.5672822e-02,\n",
              "        3.5700288e-01,  1.9112669e-01, -6.6880047e-02,  2.6036260e-01,\n",
              "       -1.5408216e-01,  2.6690418e-01, -1.0189756e-01,  9.0047577e-03,\n",
              "       -1.3924165e-01,  1.2697043e-01,  3.1512201e-01, -1.0548676e-01,\n",
              "       -6.6012651e-02, -2.4041705e-02, -6.3770786e-02, -2.2558835e-01,\n",
              "       -8.1811175e-02,  3.7635586e-01, -2.7659772e-02,  5.8677383e-02,\n",
              "        4.2305452e-01, -9.3888879e-01, -2.9948753e-01,  1.6804148e-01,\n",
              "        3.0636030e-01,  5.8549194e-04,  1.7921910e+00,  6.2060628e-02,\n",
              "        5.2705035e-02, -3.9228228e-01, -8.7544195e-02,  5.0826471e-02,\n",
              "       -3.1425327e-02,  9.3379065e-02, -1.7112350e-01, -2.7404055e-01,\n",
              "       -5.4724712e-02,  8.7006532e-02, -9.4590738e-02,  1.1274048e-01,\n",
              "        3.8350676e-03, -4.8686668e-02, -5.6254570e-03, -1.0291340e-01,\n",
              "        2.1803144e-03,  6.4524651e-02], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNJZs6-MSIcb"
      },
      "source": [
        "# vectorize all the news\n",
        "\n",
        "vectorized_text = []\n",
        "for doc in filtered_text:\n",
        "  vectorized_text.append(vectorize(doc, glove_vectors))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RAn4pDTTghs"
      },
      "source": [
        "## Clustering Documents with KMeans Cluster through Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0exKmBtT4rH"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "km = KMeans(n_clusters=20, random_state = 100).fit(vectorized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEXfA5pGUgO-",
        "outputId": "2f140051-1a31-4e57-89d9-99a61ee3262b"
      },
      "source": [
        "# view the cluster assigned to a given document, e.g., the first document:\n",
        "km.labels_[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsWjNPtTY_-T",
        "outputId": "ea44f7f2-48e2-4f7c-d786-ebd8538c65d5"
      },
      "source": [
        "# view the center of a given cluster. For example, the center of cluster 1:\n",
        "km.cluster_centers_[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.19368821e-01,  2.43030608e-01, -2.23758239e-01, -1.90863181e-01,\n",
              "        3.14634474e-01,  2.54836773e-01, -3.16943847e-01, -6.74740491e-03,\n",
              "       -1.73365647e-01,  1.80088625e-01, -9.84985662e-02,  2.01884175e-01,\n",
              "       -2.36596354e-01,  4.98275678e-02,  3.78262356e-01, -3.03294765e-02,\n",
              "        7.60515804e-02, -1.02026119e-02, -1.85542248e-01, -1.34855356e-01,\n",
              "       -2.50105073e-02,  3.75495965e-01,  3.87656810e-02,  1.08069506e-01,\n",
              "        2.99451339e-01, -1.03039050e+00, -3.82864682e-01,  1.53749177e-03,\n",
              "        1.59070822e-01, -1.08158680e-01,  1.80906229e+00,  2.01610948e-02,\n",
              "       -1.92794688e-02, -2.36382108e-01,  3.01321371e-02,  4.36473522e-02,\n",
              "       -5.28976727e-02,  5.93669506e-02, -9.80784334e-02, -2.92573140e-01,\n",
              "       -4.16237879e-02,  2.15940115e-01, -7.61974198e-02, -5.73508646e-02,\n",
              "        3.68263010e-02,  8.34009901e-04, -1.73995762e-02, -2.97494885e-01,\n",
              "       -6.14083622e-02,  1.37288397e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxCuymESXPUA"
      },
      "source": [
        "## Interpreting Clusters\n",
        "\n",
        "To interpret clusters, we usually find several most repreentative documents in one cluster and try to summarize the topics from them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AQESGisUs2E",
        "outputId": "0e3a70b4-887d-45f7-a2d0-fe3db70367f5"
      },
      "source": [
        "# For example, let's take a look at the most representative docs in cluster 1\n",
        "most_representative_docs = np.argsort(\n",
        "    np.linalg.norm(vectorized_text - km.cluster_centers_[1], axis=1)\n",
        ")\n",
        "for d in most_representative_docs[:2]:\n",
        "    print(text[d])\n",
        "    print(\"\\n ================================= \\n\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When the 1962-set \"\"On Chesil Beach\"\" begins, with British honeymooners Edward and Florence strolling along the oceanfront talking teasingly about music - he's into rock 'n' roll, she loves classical - the film hints that it might be as refreshing and uplifting as a summer breeze. The presence of Saiorse Ronan as Florence, whose disarming charms helped ground \"\"Brooklyn\"\" and \"\"Lady Bird,\"\" adds heft to that initial impression. Yet, slowly, through flashbacks of her life and that of her new husband (an impressive Billy Howle) as well as complications that happen after that pleasant walk in the sand, \"\"On Chesil Beach\"\" evolves into something much sadder and more haunting. It climaxes with a hurricane of hurt that is no doubt manipulative and melodramatic but still capable of knocking the wind out of the viewers' emotional sails. Based on Ian McEwan's novel, which was shortlisted for the Man Booker Prize, and directed by theater director Dominic Cooke, making his feature-film debut, \"\"On Chesil Beach\"\" is PBS-tasteful in its approach, but its power rests in a restraint that makes what it builds to all the more moving. Florence and Edward come from different worlds. Her doggedly, upper-middle-class English family, and especially mother Violet (Emily Watson), who wears her status like an Olympic gold medal, don't particularly care for Edward, the son of a lower-middle-class headmaster (Adrian Scarborough) and an artist (Anne-Marie Duff) left brain-damaged after a tragic run-in with a train. That Florence met Edward at a gathering for the Campaign for Nuclear Disarmament doesn't count as a selling point for her mom, who has no problem living in a perpetual state of cold war with both Russia and her daughter. The young couple perseveres, but their differences end up running much deeper than rock's wild-eyedwop-bop-a-loo-bop-a-lop-bom-bom versus classical's straitlaced sturm-und-drang . Their problems and attitudes revolving around communication, intimacy, manhood and femininity - so very much of their time and class - might seem bewilderingly anachronistic to a younger generation that can't imagine a world before Google, Oprah and self-help philosophers. But they mirror the tenor of the times. Ronan and Howle, who also appear together in another current movie, \"\"The Seagull,\"\" are finely matched, while the supporting cast - including Watson, Samuel West as Florence's father, and Bebe Cave as her sister - are pitch-perfect as well. The last time Ronan appeared in a movie adapted from a McEwan novel, it was \"\"Atonement,\"\" a 2007 film nominated for seven Oscars, including a supporting nod for the actress. Some fans of the book, which I've not read, have groused that Cooke has kept the skeletal frame of the novel but removed its beating heart, replacing it with a smothering gentility, even though McEwan himself wrote the screenplay. That may be true, but \"\"On Chesil Beach\"\" is still no carefree day at the shore.\n",
            "\n",
            " ================================= \n",
            "\n",
            "Fairy tales and feminism haven't always seen eye to eye. Damsels in distress awaiting a handsome rescuer and demure beauties pining for a Prince Charming don't fit the mold of the take-charge female. But \"\"Ever After\"\" proves that women can have their romantic visions and their empowerment, too. Drew Barrymore doesn't need help from any men - or fairy godmothers - in this \"\"realistic\"\" recasting of the Cinderella saga. Writer-director Andy Tennant makes no secret of his political motivations. The father of two daughters, he wanted to make a movie for them that embraced fairy tales' optimism without endorsing their sexism. The message is that dreams can come true, but not if you just sit around waiting for someone else to make it so. Danielle (Barrymore) lives in the 16th century but comes from the late 20th century. She has a can-do attitude melded with a strong sense of political correctness. Despite living in a culture that segregates people by gender and class, she refuses to classify anyone by their station. Her widowed father dies shortly after marrying Rodmilla (Anjelica Huston), who has two daughters of her own - Marguerite (Megan Dodds in her first significant film role) and Jacqueline (Melanie Lynskey, \"\"Heavenly Creatures\"\"). Rodmilla has no interest in or love for Danielle. Marguerite and Jacqueline live a pampered life, while Danielle is reduced to a glorified servant. Meanwhile, the king has ordered his son, Prince Henry (Dougray Scott, \"\"Deep Impact\"\"), to marry the daughter of the king of Spain in order to cement the political ties between their countries. When Henry objects on the grounds that he doesn't love the Spanish woman, his father gives him five days to come up with an alternative bride. News of Henry's search spreads quickly. Rodmilla immediately pushes her daughters to the front of the line. Henry is struck by Marguerite's beauty, and Rodmilla steps up her campaign to orchestrate a royal wedding. What she doesn't know is that Henry also met Danielle and was bowled over by her forthright attitude. When Rodmilla learns that Marguerite's competition is Danielle, she's furious. Huston rarely turns in anything less than a great performance, so it's no surprise that her rendition of the evil stepmother is right on the mark. She's so nasty as she's barking out orders that you wish Latrell Sprewell would make a cameo appearance to choke her. Barrymore's convincing performance is a bit surprising, however. The former \"\"E.T.\"\" phenom's career hit the skids with a series of bad-girl roles - both on and off screen. Now she's fighting her way through a comeback. She parlayed small gigs in \"\"Everyone Says I Love You\"\" and \"\"Scream\"\" into a featured role in \"\"The Wedding Singer.\"\" This is her first shot at playing the leading lady, and she makes the most of it with a performance that underscores her character's strength and conviction. Tennant, who made his name on TV's \"\"The Wonder Years,\"\" hasn't abandoned all the trappings of fairy tales. The movie has a luminous, romantic look enhanced by elegant settings. A note for parents: The film is rated PG-13 based primarily on a scene in which Henry utters four consecutive profanities. There's no sex or violence. The string of cursing comes as shock, not only because it is so out of step with the rest of the film but also because it seems so pointless. \n",
            "\n",
            " ================================= \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXP6UlfuVDMs",
        "outputId": "04d7e2dc-6171-4e86-f3f5-e89c6936f286"
      },
      "source": [
        "# For example, let's take a look at the most representative docs in cluster 2\n",
        "most_representative_docs = np.argsort(\n",
        "    np.linalg.norm(vectorized_text - km.cluster_centers_[2], axis=1)\n",
        ")\n",
        "for d in most_representative_docs[:2]:\n",
        "    print(text[d])\n",
        "    print(\"\\n ================================= \\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GHOST Town opens with a brief shot of a New York City street, emptied of its usual human hustle: the Big Apple as a ghost town in the traditional sense. But the image fades. And the title, we learn, refers to overpopulation. \"\"New York is lousy with ghosts,\"\" whines a recent spectral arrival, a gadabout adulterer named Frank. Portrayed by Greg Kinnear in a crisp black tux unspoiled by the bus that whacked him sideways, Frank has unfinished business with his widowed wife, Gwen (TÃ©a Leoni), who is now engaged to \"\"a real scumbag lawyer - a bad, bad guy.\"\" And he can't do a danged thing about it without Bertram Pincus, D.D.S., whom he recruits from across the ether to sabotage Gwen's upcoming marriage. Three things you must know about Bertrum Pincus. One: He is a misanthropic dentist, arguably the most misanthropic dentist to grace the screen since Little Shop of Horrors. Two: He sees dead people, lots and lots of dead people, and he appears to be the only living soul on the isle of Manhattan who can. Three: He's played by Ricky Gervais, which means both his misanthropy and his dentistry are somehow maddeningly funny. Perhaps it's that stammering inability of his to complete most any thought in normal linear fashion. Before our eyes, sentences disintegrate into a sandy clutter of fragments, hemming and hand jives. Or perhaps it's his face, that exquisite, pasty canvas of vanity and fear - a face that can accommodate blanched terror and ridiculous self-absorption simultaneously, with room to spare for loneliness, pigheadedness and the less quantifiable strangeness of an alien interloper who hasn't yet decoded earthling communications. Gervais obviously hails from another planet, or short of that, the original BBC version of The Office. But there's also a gentleness to the man - like E.T., he comes in peace - that warms up in the middle of Ghost Town, when he dials down the testy iconoclasm and joins the human race. This being a romantic comedy, sparks will fly between Bertram and Gwen. This being a supernatural comedy, pushy ghosts will drop in on Bertram at inopportune times in anachronistic (or merely absent) clothing, asking for help in pursuit of worldly closure. Much of this is familiar enough from Ghost or The Sixth Sense or, to rewind several decades, Topper - in which Constance Bennett and a tuxedoed Cary Grant were glam dead marrieds who couldn't R.I.P. until they cheered up a fuddy-duddy friend. As Frank, Kinnear has swiped both the bowtie and the scrunched brow from Grant, who, it's worth noting, was fictionalized as a suave ectoplasmic advice-giver in 2004's That Touch of Pink. (Kyle McLachlan played him, natch cum tux.) But if the plot of Ghost Town sounds like a retread, its whimsy and sweetness work like a charm. There is so much to like in this kooky, kind-hearted and often hilarious film: Leoni's spunk; Gervais' shock; a mummy-inspection bit featuring organs in canopic jars; an absurd monologue on ``self-righteous teeth.\"\" A scene between Bertram and a surgeon who performs his colonoscopy (Saturday Night Live's Kristen Wiig) is a tour de force of dissembling cross-talk and half-mumbled interruptions - to say more would ruin the fun. Here and elsewhere, the timing is delicious. With lesser talent in front of the camera or a heavier hand behind it, the film might have crashed with a thud. But in his first directorial effort since 2004's half-baked Secret Window, David Koepp (who wrote Indiana Jones and the Kingdom of the Crystal Skull) joins his bent for supernatural storytelling with a nimble comic touch. He and co-scribe John Kamps turned out a high-spirited screenplay, but then he did a wise thing - he backed off. He gave Gervais room to be Gervais, letting scenes play out with lovely absurdist rhythms. Sometimes, a smart director turns into a ghost himself.\n",
            "\n",
            " ================================= \n",
            "\n",
            "ROLE Models is not a Judd Apatow movie, but you're forgiven for thinking it is. It stars Paul Rudd, a veteran of \"\"The 40-Year-Old Virgin\"\" and \"\"Knocked Up,\"\" alongside Seann William Scott, who isn't but should have been. It features Christopher Mintz-Plasse, Superbad's spurious Hawaiian organ donor, as well as lots and lots (and lots) of booty-obsessive slacker humor that eventually gives way to warm life lessons and a late-onset, but utterly sincere, emotional maturation. The only thing missing from this Apatowannabe is Seth Rogen, now found wearing short-shorts in Zack and Miri Make a Porno. Which, come to think of it, might have been Apatow's, too: It hails from the squishy heart and potty mouth of Kevin Smith, who was Judd Apatow before Judd Apatow. And it co-stars Elizabeth Banks, Rudd's conflicted love interest in Role Models. Then again, Role Models actually generates laughter. It hits the proverbial funny bone - to name a safely G-rated body part - with welcome force and regularity, despite a few stock gimmicks and multiple screenwriters. It was directed by David Wain (from whence came the biblical strangeness of The Ten) and co-written by Wain, Rudd, Timothy Dowling and repeat Wain collaborator Ken Marino, who also pops up onscreen in a small role as a boob stepfather. The premise is straightforward enough. Danny (Rudd) and Wheeler (Scott) are a coupla schmucks who hawk Minotaur energy drink (Wheeler calls it \"\"nuclear horse\"\" wee) to school kids at anti-drug assemblies. On one particularly awful day, Danny gets rejected by his girfriend and decompresses by quaffing can after can of Minotaur, then crashing the enormously horned company pickup into an equestrian statue. As punishment, he and Wheeler are told to choose between jail time and 150 hours of community service at Sturdy Wings, a Big Brother-y program run by a live wire (the sublimely twitchy Jane Lynch) in recovery from everything. She assigns Wheeler to Ronnie (Bobb'e J. Thompson), a squirt with a bloodcurdling vocabulary. He's a riot. As one of those smut-conversant moppets so often found in comedies these days, he's also just about unquotable - aside from \"\"If you're white, then you're Ben Affleck,\"\" one of the year's best false inductions. Danny, meanwhile, gets assigned to Augie (Mintz-Plasse), a caped dweeb who comes to life in live-action fantasy role-playing punctuated by Alacks, Alays, and thwacking Styrofoam swordplay. Augie pineth for a fair maiden, and while he's never actually spoken with her, he has \"\"killed her a couple times in battle.\"\" Wain and his convocation of screenwriters play up the subcultural wackiness (I loved Ken Jeong as a vainglorious monarch in a plastic crown), but they show nothing but kindness toward Augie. They don't mock Augie for being a part of it. If Role Models is out to humiliate anyone, it's Danny and Wheeler: Danny for being a joyless and pinch-faced contrarian; Wheeler for his randiness, recklessness and platitudinous chitchat. \"\"When one door closes, another opens,\"\" he says: At last, a lech happy to paraphrase Reverend Mother in The Sound of Music. The film tosses around KISS homages (Wheeler's) and deadpan repartee (Danny's) with the same coarse verve, lending additional time to close-in reaction shots that give the humor extra kick. None of it's all that groundbreaking. No boundaries get pushed, not even the once-sacred barrier between four-letter words and peppy moralizing: Apatow helped bomb that one to rubble. Role Models crosses plenty of familiar territory, but Wain and his merry band of mischief-makers offer a likeable, laughable mix of testosterone and heart. \n",
            "\n",
            " ================================= \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}